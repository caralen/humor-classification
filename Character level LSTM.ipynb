{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import multiprocessing\n",
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "import os\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "\n",
    "char_embeddings = KeyedVectors.load_word2vec_format(\"../gensim_char-embeddings.txt\", binary=False)\n",
    "char_vectors = char_embeddings.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "glove_model = KeyedVectors.load_word2vec_format(\"../gensim_glove_vectors25.txt\", binary=False)\n",
    "vectors = glove_model.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Input, Activation, Flatten, concatenate\n",
    "from keras.layers import Conv1D, MaxPooling1D, Dropout\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Embedding, Dense, Dropout, LSTM\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(X_train, X_test, y_train, y_test):\n",
    "    # Tokenizer\n",
    "    tk = Tokenizer(num_words=None, char_level=True, oov_token='UNK')\n",
    "    tk.fit_on_texts(X_train)\n",
    "\n",
    "    \n",
    "    # alphabet=\"abcdefghijklmnopqrstuvwxyz0123456789 ,;.!?:'\\\"/\\\\|_@#$%^&*~`+-=<>()[]{}\"\n",
    "    alphabet=\"ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789 ,;.!?:'\\\"/\\\\|_@#$%^&*~`+-=<>()[]{}\"\n",
    "    char_dict = {}\n",
    "    for i, char in enumerate(alphabet):\n",
    "        char_dict[char] = i + 1\n",
    "\n",
    "        \n",
    "    # Use char_dict to replace the tk.word_index\n",
    "    tk.word_index = char_dict.copy() \n",
    "    # Add 'UNK' to the vocabulary \n",
    "    tk.word_index[tk.oov_token] = max(char_dict.values()) + 1\n",
    "\n",
    "    \n",
    "    # Convert string to index \n",
    "    train_sequences = tk.texts_to_sequences(X_train)\n",
    "    test_texts = tk.texts_to_sequences(X_test)\n",
    "\n",
    "    # Padding\n",
    "    train_data = pad_sequences(train_sequences, maxlen=300, padding='post')\n",
    "    test_data = pad_sequences(test_texts, maxlen=300, padding='post')\n",
    "\n",
    "    # Convert to numpy array\n",
    "    train_data = np.array(train_data, dtype='float32')\n",
    "    test_data = np.array(test_data, dtype='float32')\n",
    "    \n",
    "    \n",
    "    vocab_size = len(tk.word_index)\n",
    "\n",
    "    embedding_weights = [] #(97, 96)\n",
    "    embedding_weights.append(np.zeros(vocab_size)) # first row is pad\n",
    "\n",
    "    for char, i in tk.word_index.items(): # from index 1 to 70\n",
    "        onehot = np.zeros(vocab_size)\n",
    "        onehot[i-1] = 1\n",
    "        embedding_weights.append(onehot)\n",
    "            \n",
    "    embedding_weights = np.array(embedding_weights)\n",
    "    \n",
    "#     print(np.shape(y_train))\n",
    "    \n",
    "    train_classes = to_categorical(y_train)\n",
    "    test_classes = to_categorical(y_test)\n",
    "    \n",
    "    return [train_data, train_classes, test_data, test_classes, embedding_weights, vocab_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def separate_data(X, y):\n",
    "    X_win = []\n",
    "    X_top10 = []\n",
    "    X_rest = []\n",
    "    \n",
    "    for idx, line in enumerate(y):\n",
    "        if line[0] == 1:\n",
    "            X_rest.append(X[idx][:])\n",
    "        elif line[1] == 1:\n",
    "            X_top10.append(X[idx][:])\n",
    "        elif line[2] == 1:\n",
    "            X_win.append(X[idx][:])\n",
    "            \n",
    "    return X_win, X_top10, X_rest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def averageSimilarity(tokens):\n",
    "    counter = 0; suma = 0\n",
    "    \n",
    "    for w1, w2 in itertools.combinations(tokens, 2):\n",
    "        w1 = w1.lower(); w2 = w2.lower()\n",
    "        if w1 not in vectors.vocab or w2 not in vectors.vocab:\n",
    "            continue\n",
    "        suma += vectors.similarity(w1, w2)\n",
    "        counter += 1\n",
    "        \n",
    "    if counter != 0:\n",
    "        return suma /  counter\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractFeatures(X):\n",
    "    \n",
    "    features = np.zeros((1, 5))\n",
    "    \n",
    "    for tweet in X:\n",
    "        tokens = tweet.split()\n",
    "        \n",
    "        words_num = len(tweet.split())\n",
    "        chars_num = np.sum(np.array([len(token) for token in tokens]))\n",
    "        unk_num = np.sum(np.array([1 for token in tokens if token.lower() not in vectors.vocab]))\n",
    "        caps_num = np.sum(np.array([1 for token in tokens if token.isupper()]))\n",
    "        average_similarity = averageSimilarity(tokens)\n",
    "        \n",
    "        arr = np.array([words_num, chars_num, unk_num, caps_num, average_similarity]).reshape(1, 5)\n",
    "        \n",
    "        features = np.vstack((features, arr))\n",
    "    \n",
    "    return features[1:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MY_LSTM:\n",
    "    def __init__(self):\n",
    "        self.model = None\n",
    "        self.num_of_classes = 3\n",
    "        self.dropout = 0.5\n",
    "        self.loss = 'categorical_crossentropy'\n",
    "        self.optimizer = 'adam'\n",
    "        self.activation = 'sigmoid'\n",
    "        self.cnt = 0\n",
    "        self.bingo_cnt = 0\n",
    "        \n",
    "    def createModel(self, embedding_layer):\n",
    "        model = Sequential()\n",
    "        model.add(embedding_layer)\n",
    "        model.add(LSTM(128))\n",
    "        model.add(Dropout(self.dropout))\n",
    "        model.add(Dense(3, activation = self.activation))\n",
    "\n",
    "        model.compile(loss=self.loss,\n",
    "                      optimizer=self.optimizer,\n",
    "                      metrics=['accuracy'])\n",
    "        \n",
    "        self.model = model\n",
    "        \n",
    "        \n",
    "    def fit(self, train_data, train_classes):\n",
    "        \n",
    "        X_train, X_val, y_train, y_val = train_test_split(train_data, train_classes, test_size=0.1, shuffle=True)\n",
    "        \n",
    "        self.model.fit(X_train, y_train, \n",
    "                       validation_data=(X_val, y_val),\n",
    "                       batch_size=16, \n",
    "                       epochs=7, \n",
    "                       verbose=2)\n",
    "        \n",
    "    def evaluate(self, X_test, y_test):\n",
    "        score = self.model.evaluate(X_test, y_test, batch_size=16)\n",
    "        \n",
    "        X_win, X_top10, X_rest = separate_data(X_test, y_test)\n",
    "        \n",
    "        cnt = 0\n",
    "        bingo_cnt = 0\n",
    "        \n",
    "        for tweet_pair in itertools.product(X_top10, X_rest):\n",
    "            cnt += 1\n",
    "            y_top10 = self.model.predict(tweet_pair[0].reshape(1, 300))\n",
    "            y_rest = self.model.predict(tweet_pair[1].reshape(1, 300))\n",
    "\n",
    "            if y_rest[0][0] > y_top10[0][0]:\n",
    "                bingo_cnt += 1\n",
    "        \n",
    "        print('accuracy:', bingo_cnt / cnt)\n",
    "        self.cnt += cnt\n",
    "        self.bingo_cnt += bingo_cnt\n",
    "        \n",
    "        \n",
    "    def run(self, Xs, ys):\n",
    "        \n",
    "        num_hts = len(ys)\n",
    "        for i in range(num_hts):\n",
    "            \n",
    "            X_train = [*itertools.chain.from_iterable(Xs[:i] + Xs[i + 1:])]\n",
    "            y_train = [*itertools.chain.from_iterable(ys[:i] + ys[i + 1:])]\n",
    "            X_test = Xs[i]\n",
    "            y_test = ys[i]\n",
    "            \n",
    "            train_data, train_classes, test_data, test_classes, embedding_weights, vocab_size = preprocess(X_train, X_test, y_train, y_test)\n",
    "            \n",
    "            input_size = 300\n",
    "            embedding_size = 96\n",
    "            \n",
    "            # Embedding layer Initialization\n",
    "            embedding_layer = Embedding(vocab_size + 1,\n",
    "                                         embedding_size,\n",
    "                                         input_length=input_size,\n",
    "                                         weights=[embedding_weights])\n",
    "\n",
    "            self.createModel(embedding_layer)\n",
    "            self.fit(train_data, train_classes)\n",
    "            self.evaluate(test_data, test_classes)\n",
    "            \n",
    "            print('Current total accuracy:', self.bingo_cnt / self.cnt)\n",
    "            \n",
    "        print('Total accuracy:', self.bingo_cnt / self.cnt)\n",
    "        \n",
    "        \n",
    "\n",
    "class MY_CNN:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.model = None\n",
    "        self.num_of_classes = 3\n",
    "        self.dropout = 0.2\n",
    "        self.optimizer = 'adam'\n",
    "        self.loss = 'categorical_crossentropy'\n",
    "        self.cnt = 0\n",
    "        self.bingo_cnt = 0\n",
    "        \n",
    "    def createModel(self, input_size, conv_layers, fully_connected_layers):\n",
    "        # Input\n",
    "        inputs = Input(shape=(input_size,), name='input', dtype='int64')\n",
    "        \n",
    "        # Embedding \n",
    "        x = self.embedding_layer(inputs)\n",
    "        \n",
    "        # Conv \n",
    "        for filter_num, filter_size, pooling_size in conv_layers:\n",
    "            x = Conv1D(filter_num, filter_size)(x) \n",
    "            x = Activation('relu')(x)\n",
    "            if pooling_size != -1:\n",
    "                x = MaxPooling1D(pool_size=pooling_size)(x)\n",
    "        x = Flatten()(x)\n",
    "        \n",
    "        # Fully connected layers \n",
    "        for dense_size in fully_connected_layers:\n",
    "            x = Dense(dense_size, activation='relu')(x)\n",
    "            x = Dropout(self.dropout)(x)\n",
    "            \n",
    "        # Output Layer\n",
    "        predictions = Dense(self.num_of_classes, activation='softmax')(x)\n",
    "        \n",
    "        # Build model\n",
    "        model = Model(inputs=inputs, outputs=predictions)\n",
    "        model.compile(optimizer=self.optimizer, loss=self.loss, metrics=['accuracy'])\n",
    "        self.model = model\n",
    "#         self.model.summary()\n",
    "        \n",
    "        \n",
    "    def fit(self, train_data, train_classes):\n",
    "        X_train, X_val, y_train, y_val = train_test_split(train_data, train_classes, test_size=0.1, shuffle=True)\n",
    "        \n",
    "#         class_weight = {0: 10,\n",
    "#                 1: 1,\n",
    "#                 2: 1}\n",
    "        \n",
    "        self.model.fit(X_train, y_train, \n",
    "                       validation_data=(X_val, y_val),\n",
    "                       batch_size=128, \n",
    "                       epochs=2, \n",
    "                       verbose=2)\n",
    "        \n",
    "    def evaluate(self, X_test, y_test):\n",
    "        y_pred = self.model.predict(X_test)\n",
    "        \n",
    "        X_win, X_top10, X_rest = separate_data(X_test, y_test)\n",
    "        \n",
    "        cnt = 0\n",
    "        bingo_cnt = 0\n",
    "        \n",
    "        for tweet_pair in itertools.product(X_top10, X_rest):\n",
    "            cnt += 1\n",
    "            y_top10 = self.model.predict(tweet_pair[0].reshape(1, 300))\n",
    "            y_rest = self.model.predict(tweet_pair[1].reshape(1, 300))\n",
    "\n",
    "            if y_rest[0][0] > y_top10[0][0]:\n",
    "                bingo_cnt += 1\n",
    "                \n",
    "        for tweet_pair in itertools.product(X_top10, X_win):\n",
    "            cnt += 1\n",
    "            y_top10 = self.model.predict(tweet_pair[0].reshape(1, 300))\n",
    "            y_win = self.model.predict(tweet_pair[1].reshape(1, 300))\n",
    "\n",
    "            if y_top10[0][0] > y_win[0][0]:\n",
    "                bingo_cnt += 1\n",
    "                \n",
    "        for tweet_pair in itertools.product(X_rest, X_win):\n",
    "            cnt += 1\n",
    "            y_rest = self.model.predict(tweet_pair[0].reshape(1, 300))\n",
    "            y_win = self.model.predict(tweet_pair[1].reshape(1, 300))\n",
    "\n",
    "            if y_rest[0][0] > y_win[0][0]:\n",
    "                bingo_cnt += 1\n",
    "        \n",
    "        print('accuracy:', bingo_cnt / cnt)\n",
    "        self.cnt += cnt\n",
    "        self.bingo_cnt += bingo_cnt\n",
    "        \n",
    "    def run(self, Xs, ys):\n",
    "        \n",
    "        conv_layers = [[100, 5, 3],\n",
    "               [100, 5, 3]]\n",
    "\n",
    "        fully_connected_layers = [256, 256]\n",
    "        \n",
    "        num_hts = len(ys)\n",
    "        for i in range(num_hts):\n",
    "            \n",
    "            X_train = [*itertools.chain.from_iterable(Xs[:i] + Xs[i + 1:])]\n",
    "            y_train = [*itertools.chain.from_iterable(ys[:i] + ys[i + 1:])]\n",
    "            X_test = Xs[i]\n",
    "            y_test = ys[i]\n",
    "            \n",
    "            train_data, train_classes, test_data, test_classes, embedding_weights, vocab_size = preprocess(X_train, X_test, y_train, y_test)\n",
    "            \n",
    "            input_size = 300\n",
    "            embedding_size = 96\n",
    "            \n",
    "            # Embedding layer Initialization\n",
    "            self.embedding_layer = Embedding(vocab_size + 1,\n",
    "                                             embedding_size,\n",
    "                                             input_length=input_size,\n",
    "                                             weights=[embedding_weights])\n",
    "\n",
    "            self.createModel(input_size, conv_layers, fully_connected_layers)\n",
    "            self.fit(train_data, train_classes)\n",
    "            self.evaluate(test_data, test_classes)\n",
    "            \n",
    "            print('Current total accuracy:', self.bingo_cnt / self.cnt)\n",
    "            \n",
    "        print('Total accuracy:', self.bingo_cnt / self.cnt)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    def run2(self, Xs, ys, Xt, yt, h):\n",
    "        \n",
    "        conv_layers = [[100, 5, 3],\n",
    "               [100, 5, 3]]\n",
    "\n",
    "        fully_connected_layers = [256, 256]\n",
    "        \n",
    "        first = True\n",
    "        \n",
    "        num_hts = len(yt)\n",
    "        for i in range(num_hts):\n",
    "            \n",
    "            print(h[i])\n",
    "            \n",
    "            X_train = [*itertools.chain.from_iterable(Xs[:])]\n",
    "            y_train = [*itertools.chain.from_iterable(ys[:])]\n",
    "            X_test = Xt[i]\n",
    "            y_test = yt[i]\n",
    "            \n",
    "            train_data, train_classes, test_data, test_classes, embedding_weights, vocab_size = preprocess(X_train, X_test, y_train, y_test)\n",
    "            \n",
    "            input_size = 300\n",
    "            embedding_size = 96\n",
    "            \n",
    "            # Embedding layer Initialization\n",
    "            self.embedding_layer = Embedding(vocab_size + 1,\n",
    "                                             embedding_size,\n",
    "                                             input_length=input_size,\n",
    "                                             weights=[embedding_weights])\n",
    "\n",
    "            if first == True:\n",
    "                self.createModel(input_size, conv_layers, fully_connected_layers)\n",
    "                self.fit(train_data, train_classes)\n",
    "                first = False\n",
    "            self.evaluate(test_data, test_classes)\n",
    "            \n",
    "            print('Current total accuracy:', self.bingo_cnt / self.cnt)\n",
    "            \n",
    "        print('Total accuracy:', self.bingo_cnt / self.cnt)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_document(data_location, htf):\n",
    "    tweets = []\n",
    "    labels = []\n",
    "    \n",
    "    for line in open(os.path.join(data_location, htf)).readlines():\n",
    "        line_split = line.strip().split('\\t')\n",
    "        tweets.append(line_split[1])\n",
    "        labels.append(int(line_split[2]))\n",
    "\n",
    "    return {'tweets': tweets, 'labels': labels}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data(data_location):\n",
    "    ht_files = sorted(os.listdir(data_location))\n",
    "\n",
    "    Xs = []\n",
    "    ys = []\n",
    "    ht_list = []\n",
    "    for htf in ht_files:\n",
    "        ht_dict = load_document(data_location, htf)\n",
    "\n",
    "        ht_list.append(htf)\n",
    "        ys.append(ht_dict['labels'])\n",
    "        Xs.append(ht_dict['tweets'])\n",
    "        \n",
    "    return Xs, ys, ht_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bad_Job_In_5_Words.tsv\n",
      "Train on 10192 samples, validate on 1133 samples\n",
      "Epoch 1/2\n",
      " - 14s - loss: 0.3557 - acc: 0.9037 - val_loss: 0.3487 - val_acc: 0.9082\n",
      "Epoch 2/2\n",
      " - 12s - loss: 0.3095 - acc: 0.9122 - val_loss: 0.3558 - val_acc: 0.9082\n",
      "accuracy: 0.7367829021372329\n",
      "Current total accuracy: 0.7367829021372329\n",
      "Break_Up_In_5_Words.tsv\n",
      "accuracy: 0.7072838665759019\n",
      "Current total accuracy: 0.7184054283290925\n",
      "Broadway_A_Celeb.tsv\n",
      "accuracy: 0.6274509803921569\n",
      "Current total accuracy: 0.6833463643471462\n",
      "Cereal_Songs.tsv\n",
      "accuracy: 0.54421768707483\n",
      "Current total accuracy: 0.6539251952322236\n",
      "Modern_Shakespeare.tsv\n",
      "accuracy: 0.8187038556193601\n",
      "Current total accuracy: 0.6869350862777321\n",
      "Ruin_A_Christmas_Movie.tsv\n",
      "accuracy: 0.6554132712456344\n",
      "Current total accuracy: 0.6830357142857143\n",
      "Total accuracy: 0.6830357142857143\n"
     ]
    }
   ],
   "source": [
    "Xs, ys, ht_list = create_data('../train_data')\n",
    "Xt, yt, h = create_data('../gold_data')\n",
    "\n",
    "cnn = MY_CNN().run2(Xs, ys, Xt, yt, h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
