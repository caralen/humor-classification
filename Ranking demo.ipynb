{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import multiprocessing\n",
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "import os\n",
    "import itertools\n",
    "import operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Input, Activation, Flatten, concatenate\n",
    "from keras.layers import Conv1D, MaxPooling1D, Dropout\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Embedding, Dense, Dropout, LSTM\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(X_train, X_test, y_train):\n",
    "    # Tokenizer\n",
    "    tk = Tokenizer(num_words=None, char_level=True, oov_token='UNK')\n",
    "    tk.fit_on_texts(X_train)\n",
    "\n",
    "    \n",
    "    # alphabet=\"abcdefghijklmnopqrstuvwxyz0123456789 ,;.!?:'\\\"/\\\\|_@#$%^&*~`+-=<>()[]{}\"\n",
    "    alphabet=\"ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789 ,;.!?:'\\\"/\\\\|_@#$%^&*~`+-=<>()[]{}\"\n",
    "    char_dict = {}\n",
    "    for i, char in enumerate(alphabet):\n",
    "        char_dict[char] = i + 1\n",
    "\n",
    "        \n",
    "    # Use char_dict to replace the tk.word_index\n",
    "    tk.word_index = char_dict.copy() \n",
    "    # Add 'UNK' to the vocabulary \n",
    "    tk.word_index[tk.oov_token] = max(char_dict.values()) + 1\n",
    "\n",
    "    \n",
    "    # Convert string to index \n",
    "    train_sequences = tk.texts_to_sequences(X_train)\n",
    "    test_texts = tk.texts_to_sequences(X_test)\n",
    "\n",
    "    # Padding\n",
    "    train_data = pad_sequences(train_sequences, maxlen=300, padding='post')\n",
    "    test_data = pad_sequences(test_texts, maxlen=300, padding='post')\n",
    "\n",
    "    # Convert to numpy array\n",
    "    train_data = np.array(train_data, dtype='float32')\n",
    "    test_data = np.array(test_data, dtype='float32')\n",
    "    \n",
    "    \n",
    "    vocab_size = len(tk.word_index)\n",
    "\n",
    "    embedding_weights = [] #(97, 96)\n",
    "    embedding_weights.append(np.zeros(vocab_size)) # first row is pad\n",
    "\n",
    "    for char, i in tk.word_index.items(): # from index 1 to 70\n",
    "        onehot = np.zeros(vocab_size)\n",
    "        onehot[i-1] = 1\n",
    "        embedding_weights.append(onehot)\n",
    "            \n",
    "    embedding_weights = np.array(embedding_weights)\n",
    "    \n",
    "    train_classes = to_categorical(y_train)\n",
    "    \n",
    "    return [train_data, train_classes, test_data, embedding_weights, vocab_size, tk]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessTweets(X_test, tk):\n",
    "    \n",
    "    # Convert string to index \n",
    "    test_texts = tk.texts_to_sequences(X_test)\n",
    "\n",
    "    # Padding\n",
    "    test_data = pad_sequences(test_texts, maxlen=300, padding='post')\n",
    "    \n",
    "    return test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def separate_data(X, y):\n",
    "    X_win = []\n",
    "    X_top10 = []\n",
    "    X_rest = []\n",
    "    \n",
    "    for idx, line in enumerate(y):\n",
    "        if line == 0:\n",
    "            X_rest.append(X[idx][:])\n",
    "        elif line == 1:\n",
    "            X_top10.append(X[idx][:])\n",
    "        elif line == 2:\n",
    "            X_win.append(X[idx][:])\n",
    "            \n",
    "    return X_win, X_top10, X_rest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addOneToDict(dictionary, key):\n",
    "    if key in dictionary.keys():\n",
    "        dictionary[key] += 1\n",
    "    else:\n",
    "        dictionary[key] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MY_CNN:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.model = None\n",
    "        self.num_of_classes = 3\n",
    "        self.dropout = 0.2\n",
    "        self.optimizer = 'adam'\n",
    "        self.loss = 'categorical_crossentropy'\n",
    "        self.cnt = 0\n",
    "        self.bingo_cnt = 0\n",
    "        \n",
    "    def createModel(self, input_size, conv_layers, fully_connected_layers):\n",
    "        # Input\n",
    "        inputs = Input(shape=(input_size,), name='input', dtype='int64')\n",
    "        \n",
    "        # Embedding \n",
    "        x = self.embedding_layer(inputs)\n",
    "        \n",
    "        # Conv \n",
    "        for filter_num, filter_size, pooling_size in conv_layers:\n",
    "            x = Conv1D(filter_num, filter_size)(x) \n",
    "            x = Activation('relu')(x)\n",
    "            if pooling_size != -1:\n",
    "                x = MaxPooling1D(pool_size=pooling_size)(x)\n",
    "        x = Flatten()(x)\n",
    "        \n",
    "        # Fully connected layers \n",
    "        for dense_size in fully_connected_layers:\n",
    "            x = Dense(dense_size, activation='relu')(x)\n",
    "            x = Dropout(self.dropout)(x)\n",
    "            \n",
    "        # Output Layer\n",
    "        predictions = Dense(self.num_of_classes, activation='softmax')(x)\n",
    "        \n",
    "        # Build model\n",
    "        model = Model(inputs=inputs, outputs=predictions)\n",
    "        model.compile(optimizer=self.optimizer, loss=self.loss, metrics=['accuracy'])\n",
    "        self.model = model\n",
    "        \n",
    "        \n",
    "    def fit(self, train_data, train_classes):\n",
    "        \n",
    "        self.model.fit(train_data, train_classes,\n",
    "                       batch_size=128, \n",
    "                       epochs=1, \n",
    "                       verbose=1)\n",
    "        \n",
    "\n",
    "\n",
    "    def evaluate(self, X_test_sent, tk):\n",
    "        \n",
    "        X_test = preprocessTweets(X_test_sent, tk)\n",
    "        y_pred = self.model.predict(X_test)\n",
    "        \n",
    "        indexes = np.argsort(np.array(y_pred)[:, 0]).tolist()\n",
    "        \n",
    "        print()\n",
    "        for i in range(5):\n",
    "            print(\"{}: {}\".format(i+1, X_test_sent[indexes.index(i)]))\n",
    "        \n",
    "        \n",
    "    def run(self, Xs, ys, X_test):\n",
    "        \n",
    "        conv_layers = [[100, 5, 3],\n",
    "               [100, 5, 3]]\n",
    "\n",
    "        fully_connected_layers = [256, 256]\n",
    "        \n",
    "        input_size = 300\n",
    "        embedding_size = 96\n",
    "        \n",
    "        X_train = [*itertools.chain.from_iterable(Xs[:])]\n",
    "        y_train = [*itertools.chain.from_iterable(ys[:])]\n",
    "            \n",
    "        train_data, train_classes, test_data, embedding_weights, vocab_size, tk = preprocess(X_train, X_test, y_train)\n",
    "\n",
    "\n",
    "        # Embedding layer Initialization\n",
    "        self.embedding_layer = Embedding(vocab_size + 1,\n",
    "                                         embedding_size,\n",
    "                                         input_length=input_size,\n",
    "                                         weights=[embedding_weights])\n",
    "\n",
    "        self.createModel(input_size, conv_layers, fully_connected_layers)\n",
    "        self.fit(train_data, train_classes)\n",
    "        self.evaluate(X_test, tk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_document(data_location, htf):\n",
    "    tweets = []\n",
    "    labels = []\n",
    "    \n",
    "    for line in open(os.path.join(data_location, htf)).readlines():\n",
    "        line_split = line.strip().split('\\t')\n",
    "        tweets.append(line_split[1])\n",
    "        labels.append(int(line_split[2]))\n",
    "\n",
    "    return {'tweets': tweets, 'labels': labels}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_targedy(location):\n",
    "    file = open(location, \"r\")\n",
    "    lines = []\n",
    "    \n",
    "    for line in file.readlines():\n",
    "        lines.append(line.replace('\\n', ''))\n",
    "        \n",
    "    return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data(data_location):\n",
    "    ht_files = sorted(os.listdir(data_location))\n",
    "\n",
    "    Xs = []\n",
    "    ys = []\n",
    "    ht_list = []\n",
    "    for htf in ht_files:\n",
    "        ht_dict = load_document(data_location, htf)\n",
    "\n",
    "        ht_list.append(htf)\n",
    "        ys.append(ht_dict['labels'])\n",
    "        Xs.append(ht_dict['tweets'])\n",
    "        \n",
    "    return Xs, ys, ht_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/1\n",
      " 3200/11325 [=======>......................] - ETA: 11s - loss: 0.4468 - acc: 0.8816"
     ]
    }
   ],
   "source": [
    "Xs, ys, ht_list = create_data('../train_data')\n",
    "Xt2 = load_targedy('../tar_data/#TARgedy.txt')\n",
    "\n",
    "cnn = MY_CNN().run(Xs, ys, Xt2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
