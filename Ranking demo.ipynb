{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import multiprocessing\n",
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "import os\n",
    "import itertools\n",
    "import operator\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Input, Activation, Flatten, concatenate\n",
    "from keras.layers import Conv1D, MaxPooling1D, Dropout\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Embedding, Dense, Dropout, LSTM\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(X_train, X_test, y_train, y_test):\n",
    "    # Tokenizer\n",
    "    tk = Tokenizer(num_words=None, char_level=True, oov_token='UNK')\n",
    "    tk.fit_on_texts(X_train)\n",
    "\n",
    "    \n",
    "    # alphabet=\"abcdefghijklmnopqrstuvwxyz0123456789 ,;.!?:'\\\"/\\\\|_@#$%^&*~`+-=<>()[]{}\"\n",
    "    alphabet=\"ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789 ,;.!?:'\\\"/\\\\|_@#$%^&*~`+-=<>()[]{}\"\n",
    "    char_dict = {}\n",
    "    for i, char in enumerate(alphabet):\n",
    "        char_dict[char] = i + 1\n",
    "\n",
    "        \n",
    "    # Use char_dict to replace the tk.word_index\n",
    "    tk.word_index = char_dict.copy() \n",
    "    # Add 'UNK' to the vocabulary \n",
    "    tk.word_index[tk.oov_token] = max(char_dict.values()) + 1\n",
    "\n",
    "    \n",
    "    # Convert string to index \n",
    "    train_sequences = tk.texts_to_sequences(X_train)\n",
    "    test_texts = tk.texts_to_sequences(X_test)\n",
    "\n",
    "    # Padding\n",
    "    train_data = pad_sequences(train_sequences, maxlen=300, padding='post')\n",
    "    test_data = pad_sequences(test_texts, maxlen=300, padding='post')\n",
    "\n",
    "    # Convert to numpy array\n",
    "    train_data = np.array(train_data, dtype='float32')\n",
    "    test_data = np.array(test_data, dtype='float32')\n",
    "    \n",
    "    \n",
    "    vocab_size = len(tk.word_index)\n",
    "\n",
    "    embedding_weights = [] #(97, 96)\n",
    "    embedding_weights.append(np.zeros(vocab_size)) # first row is pad\n",
    "\n",
    "    for char, i in tk.word_index.items(): # from index 1 to 70\n",
    "        onehot = np.zeros(vocab_size)\n",
    "        onehot[i-1] = 1\n",
    "        embedding_weights.append(onehot)\n",
    "            \n",
    "    embedding_weights = np.array(embedding_weights)\n",
    "    \n",
    "#     print(np.shape(y_train))\n",
    "    \n",
    "    train_classes = to_categorical(y_train)\n",
    "    test_classes = to_categorical(y_test)\n",
    "    \n",
    "    return [train_data, train_classes, test_data, test_classes, embedding_weights, vocab_size, tk]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessTweet(x, tk):\n",
    "    \n",
    "    # Convert string to index \n",
    "    test_texts = tk.texts_to_sequences([x])\n",
    "\n",
    "    # Padding\n",
    "    test_data = pad_sequences(test_texts, maxlen=300, padding='post')\n",
    "    \n",
    "    return test_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def separate_data(X, y):\n",
    "    X_win = []\n",
    "    X_top10 = []\n",
    "    X_rest = []\n",
    "    \n",
    "    for idx, line in enumerate(y):\n",
    "        if line == 0:\n",
    "            X_rest.append(X[idx][:])\n",
    "        elif line == 1:\n",
    "            X_top10.append(X[idx][:])\n",
    "        elif line == 2:\n",
    "            X_win.append(X[idx][:])\n",
    "            \n",
    "    return X_win, X_top10, X_rest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addOneToDict(dictionary, key):\n",
    "    if key in dictionary.keys():\n",
    "        dictionary[key] += 1\n",
    "    else:\n",
    "        dictionary[key] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MY_CNN:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.model = None\n",
    "        self.num_of_classes = 3\n",
    "        self.dropout = 0.2\n",
    "        self.optimizer = 'adam'\n",
    "        self.loss = 'categorical_crossentropy'\n",
    "        self.cnt = 0\n",
    "        self.bingo_cnt = 0\n",
    "        \n",
    "    def createModel(self, input_size, conv_layers, fully_connected_layers):\n",
    "        # Input\n",
    "        inputs = Input(shape=(input_size,), name='input', dtype='int64')\n",
    "        \n",
    "        # Embedding \n",
    "        x = self.embedding_layer(inputs)\n",
    "        \n",
    "        # Conv \n",
    "        for filter_num, filter_size, pooling_size in conv_layers:\n",
    "            x = Conv1D(filter_num, filter_size)(x) \n",
    "            x = Activation('relu')(x)\n",
    "            if pooling_size != -1:\n",
    "                x = MaxPooling1D(pool_size=pooling_size)(x)\n",
    "        x = Flatten()(x)\n",
    "        \n",
    "        # Fully connected layers \n",
    "        for dense_size in fully_connected_layers:\n",
    "            x = Dense(dense_size, activation='relu')(x)\n",
    "            x = Dropout(self.dropout)(x)\n",
    "            \n",
    "        # Output Layer\n",
    "        predictions = Dense(self.num_of_classes, activation='softmax')(x)\n",
    "        \n",
    "        # Build model\n",
    "        model = Model(inputs=inputs, outputs=predictions)\n",
    "        model.compile(optimizer=self.optimizer, loss=self.loss, metrics=['accuracy'])\n",
    "        self.model = model\n",
    "#         self.model.summary()\n",
    "        \n",
    "        \n",
    "    def fit(self, train_data, train_classes):\n",
    "#         X_train, X_val, y_train, y_val = train_test_split(train_data, train_classes, test_size=0.1, shuffle=True)\n",
    "        \n",
    "#         class_weight = {0: 10,\n",
    "#                 1: 1,\n",
    "#                 2: 1}\n",
    "        \n",
    "        self.model.fit(train_data, train_classes,\n",
    "                       batch_size=128, \n",
    "                       epochs=1, \n",
    "                       verbose=1)\n",
    "        \n",
    "    def evaluate(self, X_test_sent, y_test_sent, tk):\n",
    "        \n",
    "        X_win, X_top10, X_rest = separate_data(X_test_sent, y_test_sent)\n",
    "        \n",
    "        results = {}\n",
    "        \n",
    "        for tweet_pair in itertools.product(X_top10, X_rest):\n",
    "            \n",
    "            y_top10 = self.model.predict(preprocessTweet(tweet_pair[0], tk).reshape(1, 300))\n",
    "            y_rest = self.model.predict(preprocessTweet(tweet_pair[1], tk).reshape(1, 300))\n",
    "\n",
    "            if y_top10[0][0] > y_rest[0][0]:\n",
    "                addOneToDict(results, tweet_pair[1])\n",
    "            else:\n",
    "                addOneToDict(results, tweet_pair[0])\n",
    "                \n",
    "        for tweet_pair in itertools.product(X_top10, X_win):\n",
    "            \n",
    "            y_top10 = self.model.predict(preprocessTweet(tweet_pair[0], tk).reshape(1, 300))\n",
    "            y_win = self.model.predict(preprocessTweet(tweet_pair[1], tk).reshape(1, 300))\n",
    "\n",
    "            if y_top10[0][0] > y_win[0][0]:\n",
    "                addOneToDict(results, tweet_pair[1])\n",
    "            else:\n",
    "                addOneToDict(results, tweet_pair[0])\n",
    "                \n",
    "        for tweet_pair in itertools.product(X_rest, X_win):\n",
    "\n",
    "            y_rest = self.model.predict(preprocessTweet(tweet_pair[0], tk).reshape(1, 300))\n",
    "            y_win = self.model.predict(preprocessTweet(tweet_pair[1], tk).reshape(1, 300))\n",
    "\n",
    "            if y_rest[0][0] > y_win[0][0]:\n",
    "                addOneToDict(results, tweet_pair[1])\n",
    "            else:\n",
    "                addOneToDict(results, tweet_pair[0])\n",
    "\n",
    "        sorted_results = sorted(results.items(), key=operator.itemgetter(1), reverse=True)\n",
    "        print()\n",
    "        for idx, tweet in enumerate(np.array(sorted_results)[:5, 0]):\n",
    "            print(\"{}: {}\".format(idx + 1, tweet))\n",
    "#         print(np.array(sorted_results)[:3, 0])\n",
    "        \n",
    "        \n",
    "    def run(self, Xs, ys, Xt, yt, h):\n",
    "        \n",
    "        conv_layers = [[100, 5, 3],\n",
    "               [100, 5, 3]]\n",
    "\n",
    "        fully_connected_layers = [256, 256]\n",
    "        \n",
    "        first = True\n",
    "        \n",
    "        num_hts = len(yt)\n",
    "        for i in range(num_hts):\n",
    "            \n",
    "            X_train = [*itertools.chain.from_iterable(Xs[:])]\n",
    "            y_train = [*itertools.chain.from_iterable(ys[:])]\n",
    "            X_test = Xt[i]\n",
    "            y_test = yt[i]\n",
    "            \n",
    "            train_data, train_classes, test_data, test_classes, embedding_weights, vocab_size, tk = preprocess(X_train, X_test, y_train, y_test)\n",
    "            \n",
    "            input_size = 300\n",
    "            embedding_size = 96\n",
    "            \n",
    "            # Embedding layer Initialization\n",
    "            self.embedding_layer = Embedding(vocab_size + 1,\n",
    "                                             embedding_size,\n",
    "                                             input_length=input_size,\n",
    "                                             weights=[embedding_weights])\n",
    "\n",
    "            if first == True:\n",
    "                self.createModel(input_size, conv_layers, fully_connected_layers)\n",
    "                self.fit(train_data, train_classes)\n",
    "                first = False\n",
    "            self.evaluate(X_test, y_test, tk)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_document(data_location, htf):\n",
    "    tweets = []\n",
    "    labels = []\n",
    "    \n",
    "    for line in open(os.path.join(data_location, htf)).readlines():\n",
    "        line_split = line.strip().split('\\t')\n",
    "        tweets.append(line_split[1])\n",
    "        labels.append(int(line_split[2]))\n",
    "\n",
    "    return {'tweets': tweets, 'labels': labels}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "import preprocessor as p\n",
    "\n",
    "import re\n",
    "\n",
    "def ukloni_topic(tweet, topic):\n",
    "    pattern = re.compile(\"#\" + topic.replace(\"_\", \"\"), re.IGNORECASE)\n",
    "    tweet = pattern.sub(\"\", tweet).strip()\n",
    "    pattern = re.compile(\"@\" + topic.replace(\"_\", \"\"), re.IGNORECASE)\n",
    "    tweet = pattern.sub(\"\", tweet).strip()\n",
    "    return tweet\n",
    "\n",
    "def tin_preprocessing(tweets,topic):\n",
    "    result = []\n",
    "    for tweet in tweets: \n",
    "        tweet = ukloni_topic(tweet, topic)\n",
    "        result.append(tweet)    \n",
    "    return result\n",
    "\n",
    "def create_data(data_location):\n",
    "    ht_files = sorted(os.listdir(data_location))\n",
    "\n",
    "    Xs = []\n",
    "    ys = []\n",
    "    ht_list = []\n",
    "    for htf in ht_files:\n",
    "        ht_dict = load_document(data_location, htf)\n",
    "\n",
    "        ht_list.append(htf)\n",
    "        ys.append(ht_dict['labels'])\n",
    "        Xs.append(tin_preprocessing(ht_dict['tweets'],htf.replace(\"_\", \"\")))\n",
    "        \n",
    "    return Xs, ys, ht_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "11325/11325 [==============================] - 18s 2ms/step - loss: 0.3541 - acc: 0.9088\n",
      "\n",
      "1: Geting a resource error after 10 hours of training @midnight #TARgedy\n",
      "2: Writes a paper without mentioning #SotA 48 times. Is it really a paper? #TARgedy @midnight\n",
      "3: What is your orientation? Bi-directional LSTM #BiLSTM #TARgedy @midnight\n",
      "4: Scoring this tweet as the funniest #TARgedy @midnight\n",
      "5: Well, at least when it comes to the paper, the size doesn't matter @midnight #TARgedy\n"
     ]
    }
   ],
   "source": [
    "Xs, ys, ht_list = create_data('../train_data')\n",
    "Xt, yt, h = create_data('../tar_data')\n",
    "\n",
    "cnn = MY_CNN().run(Xs, ys, Xt, yt, h)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
