{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import multiprocessing\n",
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "from os import listdir\n",
    "from os.path import isfile, join"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gensim GloVe word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1193514, 25)"
      ]
     },
     "execution_count": 390,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "glove2word2vec(glove_input_file=\"glove.twitter.27B/glove.twitter.27B.25d.txt\",\n",
    "               word2vec_output_file=\"gensim_glove_vectors50.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "glove_model = KeyedVectors.load_word2vec_format(\"gensim_glove_vectors50.txt\", binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('twitter', 0.8885936141014099),\n",
       " ('instagram', 0.8861387968063354),\n",
       " ('tweet', 0.8799343109130859),\n",
       " ('tweets', 0.8788758516311646),\n",
       " ('youtube', 0.8644436597824097),\n",
       " ('facebook', 0.8608332276344299),\n",
       " ('snapchat', 0.8595442175865173),\n",
       " ('chat', 0.8593137860298157),\n",
       " ('insta', 0.8528122305870056),\n",
       " ('spam', 0.8513364791870117)]"
      ]
     },
     "execution_count": 450,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_model.wv.similar_by_word('hashtag')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = glove_model.wv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import itertools\n",
    "from random import random\n",
    "import csv\n",
    "from collections import defaultdict\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SupervisedExpRunner(object):\n",
    "    def __init__(self):\n",
    "        self.model = None\n",
    "\n",
    "    def _create_classifier(self):\n",
    "        self.model = AdaBoostClassifier(n_estimators=50)\n",
    "\n",
    "    def _fit(self, X, y):\n",
    "        if self.model is None:\n",
    "            self._create_classifier()\n",
    "\n",
    "        self.model.fit(X, y)\n",
    "        \n",
    "    def _separate_data(self, X, y):\n",
    "        X_win = X[y == 2]\n",
    "        X_top10 = X[y == 1]\n",
    "        X_rest = X[(y != 1) & (y != 2)]\n",
    "        return X_win, X_top10, X_rest\n",
    "        \n",
    "    def _evaluate(self, X, y):\n",
    "        np.set_printoptions(threshold=sys.maxsize)\n",
    "        y_pred = self.model.predict(X)\n",
    "#         print(y_pred)\n",
    "#         print(y)\n",
    "        acc = accuracy_score(y, y_pred)\n",
    "        self.results = {'accuracy': acc}\n",
    "        \n",
    "    def _create_pairwise_data(self, Xs, ys):\n",
    "        X_pairs = []\n",
    "        y_pairs = []\n",
    "        for X, y in zip(Xs, ys):\n",
    "            X_win, X_top10, X_rest = self._separate_data(X, [y])\n",
    "\n",
    "            for tweet_pair in itertools.product(X_win, X_top10):\n",
    "                if random() > 0.5:\n",
    "                    tweet_data = np.hstack((tweet_pair[0], tweet_pair[1]))\n",
    "                    tweet_label = 1\n",
    "                else:\n",
    "                    tweet_data = np.hstack((tweet_pair[1], tweet_pair[0]))\n",
    "                    tweet_label = 0\n",
    "\n",
    "                X_pairs.append(tweet_data)\n",
    "                y_pairs.append(tweet_label)\n",
    "\n",
    "            for tweet_pair in itertools.product(X_top10, X_rest):\n",
    "                if random() > 0.5:\n",
    "                    tweet_data = np.hstack((tweet_pair[0], tweet_pair[1]))\n",
    "                    tweet_label = 1\n",
    "                else:\n",
    "                    tweet_data = np.hstack((tweet_pair[1], tweet_pair[0]))\n",
    "                    tweet_label = 0\n",
    "\n",
    "                X_pairs.append(tweet_data)\n",
    "                y_pairs.append(tweet_label)\n",
    "\n",
    "            for tweet_pair in itertools.product(X_win, X_rest):\n",
    "                if random() > 0.5:\n",
    "                    tweet_data = np.hstack((tweet_pair[0], tweet_pair[1]))\n",
    "                    tweet_label = 1\n",
    "                else:\n",
    "                    tweet_data = np.hstack((tweet_pair[1], tweet_pair[0]))\n",
    "                    tweet_label = 0\n",
    "\n",
    "                X_pairs.append(tweet_data)\n",
    "                y_pairs.append(tweet_label)\n",
    "                \n",
    "        X = np.vstack(X_pairs)\n",
    "        y = np.array(y_pairs)\n",
    "\n",
    "        return X, y\n",
    "\n",
    "    def get_results(self):\n",
    "        return self.results\n",
    "    \n",
    "    def run_loo_exp(self, Xs, ys, ht_list, ow_name = 'results'):\n",
    "        out_file = open(ow_name+'.csv', 'w')\n",
    "        ow = csv.writer( out_file )\n",
    "        micro_sum = 0\n",
    "        total_pairs = 0\n",
    "        num_hts = len(ys)\n",
    "        for i in range(num_hts):\n",
    "            print(str(100*i/num_hts)+'% done')\n",
    "            \n",
    "#             X_test = Xs[i]\n",
    "#             y_test = ys[i]\n",
    "#             X_train = [*itertools.chain.from_iterable(Xs[:i] + Xs[i + 1:])]\n",
    "#             y_train = [*itertools.chain.from_iterable(ys[:i] + ys[i + 1:])]\n",
    "\n",
    "            Xs_test = [Xs[i]]\n",
    "            ys_test = [ys[i]]\n",
    "            Xs_train = Xs[:i] + Xs[i+1:]\n",
    "            ys_train = ys[:i] + ys[i+1:]\n",
    "            \n",
    "            X_train, y_train = self._create_pairwise_data(Xs_train, ys_train)\n",
    "            X_test, y_test = self._create_pairwise_data(Xs_test, ys_test)\n",
    "\n",
    "            self._fit(X_train, y_train)\n",
    "            self._evaluate(X_test, y_test)\n",
    "            ht_result = self.get_results()\n",
    "            ow.writerow([ht_list[i], str(ht_result['accuracy'])])\n",
    "            print(ht_result)\n",
    "            micro_sum += ht_result['accuracy'] * y_test.shape[0]\n",
    "            total_pairs += y_test.shape[0]\n",
    "            self.model = None\n",
    "            \n",
    "        out_file.close()\n",
    "        print('100% done')\n",
    "        return micro_sum / total_pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data(data_location):\n",
    "    ht_files = sorted(os.listdir(data_location))\n",
    "\n",
    "    Xs = []\n",
    "    ys = []\n",
    "    ht_list = []\n",
    "    for htf in ht_files:\n",
    "        ht_dict = load_hashtag(data_location,htf)\n",
    "\n",
    "        ht_list.append(htf)\n",
    "        ys.append(ht_dict['Y'])\n",
    "        Xs.append(ht_dict['X_bow'])\n",
    "\n",
    "    return Xs, ys, ht_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_hashtag(data_location, htf):\n",
    "    tweets = []\n",
    "    labels = []\n",
    "    for line in open(os.path.join(data_location,htf)).readlines():\n",
    "        line_split = line.strip().split('\\t')\n",
    "        tweets.append(line_split[1])\n",
    "        labels.append(int(line_split[2]))\n",
    "\n",
    "    Y = np.array(labels)\n",
    "    X_bow = [create_glove_sent(tweet) for tweet in tweets]\n",
    "\n",
    "    return {'X_bow':X_bow,'Y':Y}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bow_rep( in_tweet ):\n",
    "    bow_map = defaultdict(int)\n",
    "    tokens = in_tweet.split()\n",
    "    for tok in tokens:\n",
    "        bow_map[tok.lower()] += 1\n",
    "    return bow_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_glove_sent( in_tweet ):\n",
    "    tokens = in_tweet.split()\n",
    "    \n",
    "    arr = np.zeros(25)\n",
    "    \n",
    "    for tok in tokens:\n",
    "        try:\n",
    "            arr += vectors[tok]\n",
    "        except:\n",
    "            pass\n",
    "    return arr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 530,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0% done\n",
      "{'accuracy': 0.5376}\n",
      "20.0% done\n",
      "{'accuracy': 0.6218666666666667}\n",
      "40.0% done\n",
      "{'accuracy': 0.5781333333333334}\n",
      "60.0% done\n",
      "{'accuracy': 0.6058666666666667}\n",
      "80.0% done\n",
      "{'accuracy': 0.4928}\n",
      "100% done\n",
      "micro-avergae accuracy: 0.5672533333333334\n"
     ]
    }
   ],
   "source": [
    "Xs, ys, ht_list = create_data('trial_data')\n",
    "\n",
    "exp_runner = SupervisedExpRunner()\n",
    "outwriter_name = 'results_sem'\n",
    "results = exp_runner.run_loo_exp(Xs, ys, ht_list, outwriter_name)\n",
    "print('micro-avergae accuracy:',results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
