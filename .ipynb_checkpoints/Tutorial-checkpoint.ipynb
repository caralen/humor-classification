{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import multiprocessing\n",
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "import os\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "\n",
    "char_embeddings = KeyedVectors.load_word2vec_format(\"../gensim_char-embeddings.txt\", binary=False)\n",
    "char_vectors = char_embeddings.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Input, Activation, Flatten\n",
    "from keras.layers import Conv1D, MaxPooling1D, Dropout\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Embedding, Dense, Dropout, LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_document(data_location, htf):\n",
    "    tweets = []\n",
    "    labels = []\n",
    "    \n",
    "    for line in open(os.path.join(data_location, htf)).readlines():\n",
    "        line_split = line.strip().split('\\t')\n",
    "        tweets.append(line_split[1])\n",
    "        labels.append(int(line_split[2]))\n",
    "\n",
    "    return {'tweets': tweets, 'labels': labels}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data(data_location):\n",
    "    ht_files = sorted(os.listdir(data_location))\n",
    "\n",
    "    Xs = []\n",
    "    ys = []\n",
    "    ht_list = []\n",
    "    for htf in ht_files:\n",
    "        ht_dict = load_document(data_location, htf)\n",
    "\n",
    "        ht_list.append(htf)\n",
    "        ys.append(ht_dict['labels'])\n",
    "        Xs.append(ht_dict['tweets'])\n",
    "\n",
    "    X_train = np.array([*itertools.chain.from_iterable(Xs[1:])])\n",
    "    y_train = np.array([*itertools.chain.from_iterable(ys[1:])])\n",
    "    X_test = Xs[1]\n",
    "    y_test = ys[1]\n",
    "    \n",
    "    y_train[y_train == 2] = 1\n",
    "    y_test[y_test == 2] = 1\n",
    "        \n",
    "    return X_train, y_train, X_test, y_test, ht_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_test, y_test, ht_list = create_data('../trial_data')\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Tokenizer\n",
    "tk = Tokenizer(num_words=None, char_level=True, oov_token='UNK')\n",
    "tk.fit_on_texts(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alphabet=\"abcdefghijklmnopqrstuvwxyz0123456789 ,;.!?:'\\\"/\\\\|_@#$%^&*~`+-=<>()[]{}\"\n",
    "alphabet=\"ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789 ,;.!?:'\\\"/\\\\|_@#$%^&*~`+-=<>()[]{}\"\n",
    "char_dict = {}\n",
    "for i, char in enumerate(alphabet):\n",
    "    char_dict[char] = i + 1\n",
    "    \n",
    "# Use char_dict to replace the tk.word_index\n",
    "tk.word_index = char_dict.copy() \n",
    "# Add 'UNK' to the vocabulary \n",
    "tk.word_index[tk.oov_token] = max(char_dict.values()) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert string to index \n",
    "train_sequences = tk.texts_to_sequences(X_train)\n",
    "test_texts = tk.texts_to_sequences(X_test)\n",
    "\n",
    "# Padding\n",
    "train_data = pad_sequences(train_sequences, maxlen=300, padding='post')\n",
    "test_data = pad_sequences(test_texts, maxlen=300, padding='post')\n",
    "\n",
    "# Convert to numpy array\n",
    "train_data = np.array(train_data, dtype='float32')\n",
    "test_data = np.array(test_data, dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(tk.word_index)\n",
    "\n",
    "embedding_weights = [] #(97, 96)\n",
    "embedding_weights.append(np.zeros(vocab_size)) # first row is pad\n",
    "\n",
    "for char, i in tk.word_index.items(): # from index 1 to 70\n",
    "    onehot = np.zeros(vocab_size)\n",
    "    onehot[i-1] = 1\n",
    "    embedding_weights.append(onehot)\n",
    "embedding_weights = np.array(embedding_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "\n",
    "train_classes = to_categorical(y_train)\n",
    "test_classes = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameter \n",
    "input_size = 300\n",
    "# vocab_size = 69\n",
    "embedding_size = 96\n",
    "\n",
    "num_of_classes = 2\n",
    "dropout_p = 0.5\n",
    "optimizer = 'adam'\n",
    "loss = 'binary_crossentropy'\n",
    "\n",
    "conv_layers = [[256, 7, 3],\n",
    "               [256, 7, 3],\n",
    "               [256, 3, -1],\n",
    "               [256, 3, -1],\n",
    "               [256, 3, -1],\n",
    "               [256, 3, 3]]\n",
    "\n",
    "fully_connected_layers = [1024, 1024]\n",
    "\n",
    "# Embedding layer Initialization\n",
    "embedding_layer = Embedding(vocab_size+1, \n",
    "                            embedding_size,\n",
    "                            input_length=input_size,\n",
    "                            weights=[embedding_weights])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "# Model \n",
    "\n",
    "# Input\n",
    "inputs = Input(shape=(input_size,), name='input', dtype='int64')  # shape=(?, 1014)\n",
    "# Embedding \n",
    "x = embedding_layer(inputs)\n",
    "# Conv \n",
    "for filter_num, filter_size, pooling_size in conv_layers:\n",
    "    x = Conv1D(filter_num, filter_size)(x) \n",
    "    x = Activation('relu')(x)\n",
    "    if pooling_size != -1:\n",
    "        x = MaxPooling1D(pool_size=pooling_size)(x) # Final shape=(None, 34, 256)\n",
    "x = Flatten()(x) # (None, 8704)\n",
    "# Fully connected layers \n",
    "for dense_size in fully_connected_layers:\n",
    "    x = Dense(dense_size, activation='relu')(x) # dense_size == 1024\n",
    "    x = Dropout(dropout_p)(x)\n",
    "# Output Layer\n",
    "predictions = Dense(2, activation='softmax')(x)\n",
    "# Build model\n",
    "model = Model(inputs=inputs, outputs=predictions)\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy']) # Adam, categorical_crossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      " - 3s - loss: 0.2845 - acc: 0.9273\n",
      "Epoch 2/5\n",
      " - 3s - loss: 0.2699 - acc: 0.9273\n",
      "Epoch 3/5\n",
      " - 3s - loss: 0.2528 - acc: 0.9273\n",
      "Epoch 4/5\n",
      " - 3s - loss: 0.2373 - acc: 0.9273\n",
      "Epoch 5/5\n",
      " - 3s - loss: 0.2459 - acc: 0.9273\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1]\n",
      "[[0.89430815 0.10569185]\n",
      " [0.8880926  0.11190744]\n",
      " [0.87364614 0.12635386]\n",
      " [0.9407357  0.05926429]\n",
      " [0.9551587  0.04484131]\n",
      " [0.89123946 0.10876048]\n",
      " [0.8781354  0.12186453]\n",
      " [0.8967027  0.10329729]\n",
      " [0.8766388  0.12336116]\n",
      " [0.87044525 0.12955478]\n",
      " [0.90878403 0.09121599]\n",
      " [0.91124123 0.08875876]\n",
      " [0.94026583 0.05973413]\n",
      " [0.89431006 0.10568997]\n",
      " [0.8845927  0.11540724]\n",
      " [0.90086675 0.09913322]\n",
      " [0.8477988  0.15220125]\n",
      " [0.9006313  0.09936864]\n",
      " [0.8870207  0.1129793 ]\n",
      " [0.8714401  0.12855984]\n",
      " [0.9225977  0.07740232]\n",
      " [0.87313974 0.12686026]\n",
      " [0.88917756 0.11082247]\n",
      " [0.9271397  0.07286034]\n",
      " [0.9194179  0.08058206]\n",
      " [0.9383751  0.06162489]\n",
      " [0.9207627  0.07923725]\n",
      " [0.95314866 0.04685134]\n",
      " [0.96387    0.03612998]\n",
      " [0.91575164 0.08424838]\n",
      " [0.9774469  0.02255307]\n",
      " [0.88517463 0.11482535]\n",
      " [0.9041732  0.09582683]\n",
      " [0.9086419  0.09135812]\n",
      " [0.8489024  0.15109766]\n",
      " [0.8873304  0.11266953]\n",
      " [0.8473004  0.15269963]\n",
      " [0.98676467 0.01323531]\n",
      " [0.8914306  0.10856938]\n",
      " [0.9096465  0.09035347]\n",
      " [0.8460512  0.15394874]\n",
      " [0.8430241  0.1569759 ]\n",
      " [0.9071375  0.09286251]\n",
      " [0.8797081  0.1202919 ]\n",
      " [0.8744822  0.1255178 ]\n",
      " [0.9124604  0.08753964]\n",
      " [0.88167083 0.11832917]\n",
      " [0.99702126 0.00297878]\n",
      " [0.8885371  0.11146288]\n",
      " [0.90499866 0.09500141]\n",
      " [0.8695239  0.13047615]\n",
      " [0.8847691  0.11523093]\n",
      " [0.9042663  0.09573375]\n",
      " [0.9572721  0.04272795]\n",
      " [0.9072218  0.09277825]\n",
      " [0.84789425 0.15210567]\n",
      " [0.9002883  0.09971177]\n",
      " [0.93358785 0.06641217]\n",
      " [0.924326   0.07567403]\n",
      " [0.8474413  0.15255874]\n",
      " [0.8994196  0.10058035]\n",
      " [0.8695686  0.13043146]\n",
      " [0.91326284 0.08673709]\n",
      " [0.90884453 0.09115545]\n",
      " [0.8501715  0.14982852]\n",
      " [0.8959774  0.10402265]\n",
      " [0.8624818  0.13751823]\n",
      " [0.9263134  0.07368656]\n",
      " [0.9030535  0.09694649]\n",
      " [0.89114213 0.10885782]\n",
      " [0.88451624 0.11548372]\n",
      " [0.8941774  0.10582265]\n",
      " [0.9541415  0.04585849]\n",
      " [0.9449736  0.05502646]\n",
      " [0.9126695  0.08733055]\n",
      " [0.90818185 0.09181809]\n",
      " [0.92826253 0.07173754]\n",
      " [0.9375353  0.06246475]\n",
      " [0.94194037 0.0580597 ]\n",
      " [0.87408274 0.12591724]\n",
      " [0.88210195 0.11789799]\n",
      " [0.93392575 0.06607426]\n",
      " [0.9312388  0.06876127]\n",
      " [0.8864412  0.11355885]\n",
      " [0.8924065  0.10759345]\n",
      " [0.8880356  0.11196435]\n",
      " [0.8885085  0.11149145]\n",
      " [0.8494836  0.15051639]\n",
      " [0.95480585 0.04519415]\n",
      " [0.90154797 0.09845203]\n",
      " [0.890102   0.10989796]\n",
      " [0.88139135 0.1186086 ]\n",
      " [0.8907382  0.10926185]\n",
      " [0.874135   0.12586497]\n",
      " [0.87257665 0.12742342]\n",
      " [0.9306822  0.0693178 ]\n",
      " [0.9237544  0.07624567]\n",
      " [0.8761144  0.12388556]\n",
      " [0.865473   0.13452701]\n",
      " [0.8993041  0.10069583]\n",
      " [0.89320683 0.10679319]\n",
      " [0.9077933  0.09220674]\n",
      " [0.85573494 0.14426501]\n",
      " [0.90674233 0.09325765]\n",
      " [0.92288697 0.07711297]\n",
      " [0.9818309  0.01816905]\n",
      " [0.9027469  0.09725303]\n",
      " [0.9536149  0.04638513]\n",
      " [0.8827327  0.11726728]\n",
      " [0.90503144 0.0949686 ]\n",
      " [0.85716814 0.14283186]\n",
      " [0.9432151  0.05678486]\n",
      " [0.9436337  0.05636634]\n",
      " [0.8864523  0.11354772]\n",
      " [0.8564988  0.14350125]\n",
      " [0.8670929  0.13290706]\n",
      " [0.89800596 0.10199405]\n",
      " [0.85113364 0.14886637]\n",
      " [0.85714525 0.14285469]\n",
      " [0.88008124 0.11991873]\n",
      " [0.8642788  0.1357212 ]\n",
      " [0.86871713 0.13128287]\n",
      " [0.8489735  0.15102652]]\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "model.fit(train_data, train_classes,\n",
    "          batch_size=128,\n",
    "          epochs=5,\n",
    "          verbose=2)\n",
    "\n",
    "y_pred = model.predict(test_data)\n",
    "\n",
    "print(y_test)\n",
    "print(y_pred)\n",
    "# print(np.hstack((y_test.reshape(len(y_test), 1), y_pred)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
