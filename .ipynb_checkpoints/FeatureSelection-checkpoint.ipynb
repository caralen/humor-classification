{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 477,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import multiprocessing\n",
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gensim GloVe word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1193514, 25)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "glove2word2vec(glove_input_file=\"glove.twitter.27B/glove.twitter.27B.25d.txt\",\n",
    "               word2vec_output_file=\"gensim_glove_vectors25.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "glove_model = KeyedVectors.load_word2vec_format(\"gensim_glove_vectors25.txt\", binary=False)\n",
    "vectors = glove_model.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "glove2word2vec(glove_input_file=\"char-embeddings.txt\",\n",
    "               word2vec_output_file=\"gensim_char-embeddings.txt\")\n",
    "\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "char_embeddings = KeyedVectors.load_word2vec_format(\"gensim_char-embeddings.txt\", binary=False)\n",
    "char_vectors = char_embeddings.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('lil', 0.9541467428207397),\n",
       " ('bitch', 0.9520374536514282),\n",
       " ('mf', 0.9506527781486511),\n",
       " ('bruh', 0.9500958919525146),\n",
       " ('dawg', 0.9483038187026978),\n",
       " ('fuckin', 0.9452493786811829),\n",
       " ('shawty', 0.9435693025588989),\n",
       " ('ass', 0.9413027763366699),\n",
       " ('cuz', 0.9359691143035889),\n",
       " ('shit', 0.9340022802352905)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_model.wv.similar_by_word('nigga')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.02522685168689944"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectors.similarity('nigga', 'alen')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "import itertools\n",
    "from sklearn.ensemble import AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def separate_data(X, y):\n",
    "        X_win = X[y == 2]\n",
    "        X_top10 = X[y == 1]\n",
    "        X_rest = X[(y != 1) & (y != 2)]\n",
    "        return X_win, X_top10, X_rest\n",
    "    \n",
    "def create_pairwise_data(Xs, ys):\n",
    "        X_pairs = []\n",
    "        y_pairs = []\n",
    "        for X, y in zip(Xs, ys):\n",
    "            X_win, X_top10, X_rest = separate_data(X, [y])\n",
    "\n",
    "            for tweet_pair in itertools.product(X_win, X_top10):\n",
    "                if random() > 0.5:\n",
    "                    tweet_data = np.hstack((tweet_pair[0], tweet_pair[1]))\n",
    "                    tweet_label = 1\n",
    "                else:\n",
    "                    tweet_data = np.hstack((tweet_pair[1], tweet_pair[0]))\n",
    "                    tweet_label = 0\n",
    "\n",
    "                X_pairs.append(tweet_data)\n",
    "                y_pairs.append(tweet_label)\n",
    "\n",
    "            for tweet_pair in itertools.product(X_top10, X_rest):\n",
    "                if random() > 0.5:\n",
    "                    tweet_data = np.hstack((tweet_pair[0], tweet_pair[1]))\n",
    "                    tweet_label = 1\n",
    "                else:\n",
    "                    tweet_data = np.hstack((tweet_pair[1], tweet_pair[0]))\n",
    "                    tweet_label = 0\n",
    "\n",
    "                X_pairs.append(tweet_data)\n",
    "                y_pairs.append(tweet_label)\n",
    "\n",
    "            for tweet_pair in itertools.product(X_win, X_rest):\n",
    "                if random() > 0.5:\n",
    "                    tweet_data = np.hstack((tweet_pair[0], tweet_pair[1]))\n",
    "                    tweet_label = 1\n",
    "                else:\n",
    "                    tweet_data = np.hstack((tweet_pair[1], tweet_pair[0]))\n",
    "                    tweet_label = 0\n",
    "\n",
    "                X_pairs.append(tweet_data)\n",
    "                y_pairs.append(tweet_label)\n",
    "\n",
    "        X = np.vstack(X_pairs)\n",
    "        y = np.array(y_pairs)\n",
    "\n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelectionModel(object):\n",
    "    def __init__(self):\n",
    "        self.model = None\n",
    "\n",
    "    def _create_classifier(self):\n",
    "        self.model = Pipeline([\n",
    "                      ('feature_selection', SelectFromModel(LinearSVC(penalty=\"l1\"))),\n",
    "                      ('classification', RandomForestClassifier())\n",
    "                    ])\n",
    "        self.model = LogisticRegression()\n",
    "\n",
    "    def _fit(self, X, y):\n",
    "        if self.model is None:\n",
    "            self._create_classifier()\n",
    "        self.model.fit(X, y)\n",
    "        \n",
    "    def _evaluate(self, X, y):\n",
    "#         np.set_printoptions(threshold=sys.maxsize)\n",
    "        y_pred = self.model.predict(X)\n",
    "#         print(y)\n",
    "#         print(y_pred)\n",
    "        acc = accuracy_score(y, y_pred)\n",
    "        self.results = {'accuracy': acc}\n",
    "        \n",
    "    def get_results(self):\n",
    "        return self.results\n",
    "    \n",
    "    def run(self, Xs, ys, ht_list, ow_name = 'results'):\n",
    "        num_hts = len(ys)\n",
    "        for i in range(num_hts):\n",
    "            print(str(100*i/num_hts)+'% done')\n",
    "            \n",
    "            X_test = Xs[i]\n",
    "            y_test = ys[i]\n",
    "            X_train = [*itertools.chain.from_iterable(Xs[:i] + Xs[i + 1:])]\n",
    "            y_train = [*itertools.chain.from_iterable(ys[:i] + ys[i + 1:])]\n",
    "\n",
    "            self._fit(X_train, y_train)\n",
    "            self._evaluate(X_test, y_test)\n",
    "            ht_result = self.get_results()\n",
    "            print(ht_result)\n",
    "            self.model = None\n",
    "            \n",
    "        print('100% done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 523,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "from xgboost import XGBClassifier\n",
    "from xgboost import plot_importance\n",
    "from matplotlib import pyplot\n",
    "from random import random\n",
    "\n",
    "class XGBoost(object):\n",
    "    def __init__(self):\n",
    "        self.model = None\n",
    "        self.acc = 0\n",
    "        self.counter = 0\n",
    "\n",
    "    def _create_classifier(self):\n",
    "        self.model = XGBClassifier()\n",
    "        \n",
    "    def _fit(self, X, y):\n",
    "        if self.model is None:\n",
    "            self._create_classifier()\n",
    "        self.model.fit(X, y)\n",
    "        \n",
    "#         plot_importance(self.model)\n",
    "#         pyplot.show()\n",
    "#         print(self.model.get_booster().get_score(importance_type = 'total_gain'))\n",
    "        \n",
    "    def _evaluate_pairwise(self, X, y):\n",
    "        X_pairs = []\n",
    "        y_pairs = []\n",
    "        X_win, X_top10, X_rest = separate_data(X, y)\n",
    "        \n",
    "        count_corect = 0\n",
    "        count_total = 0\n",
    "        \n",
    "        for tweet_pair in itertools.product(X_top10, X_rest):\n",
    "            count_total += 1\n",
    "            \n",
    "            win_zeroclass_prob = self.model.predict_proba([tweet_pair[0]])[:,0]\n",
    "            rest_zeroclass_prob = self.model.predict_proba([tweet_pair[1]])[:,0]\n",
    "            \n",
    "            if win_zeroclass_prob < rest_zeroclass_prob:\n",
    "                count_corect += 1\n",
    "                \n",
    "        for tweet_pair in itertools.product(X_win, X_rest):\n",
    "            count_total += 1\n",
    "            \n",
    "            win_zeroclass_prob = self.model.predict_proba([tweet_pair[0]])[:,0]\n",
    "            rest_zeroclass_prob = self.model.predict_proba([tweet_pair[1]])[:,0]\n",
    "            \n",
    "            if win_zeroclass_prob < rest_zeroclass_prob:\n",
    "                count_corect += 1\n",
    "                \n",
    "        for tweet_pair in itertools.product(X_win, X_top10):\n",
    "            count_total += 1\n",
    "            \n",
    "            win_zeroclass_prob = self.model.predict_proba([tweet_pair[0]])[:,0]\n",
    "            rest_zeroclass_prob = self.model.predict_proba([tweet_pair[1]])[:,0]\n",
    "            \n",
    "            if win_zeroclass_prob < rest_zeroclass_prob:\n",
    "                count_corect += 1\n",
    "                \n",
    "        accuracy = count_corect / float(count_total)\n",
    "                \n",
    "        self.counter += 1\n",
    "        self.acc += accuracy\n",
    "                \n",
    "        print('pairwise_accuracy', accuracy)\n",
    "        \n",
    "    def _evaluate(self, X, y):\n",
    "#         np.set_printoptions(threshold=sys.maxsize)\n",
    "        self._evaluate_pairwise(X, y)\n",
    "#         y_pred = self.model.predict(X)\n",
    "#         acc = accuracy_score(y, y_pred)\n",
    "#         self.results = {'accuracy': acc}\n",
    "        \n",
    "    def get_results(self):\n",
    "        return self.results\n",
    "    \n",
    "    def run(self, Xs, ys, ht_list):\n",
    "        num_hts = len(ys)\n",
    "        for i in range(num_hts):\n",
    "            print(str(i / float(num_hts))+'% done')\n",
    "            print(ht_list[i])\n",
    "            \n",
    "            X_test = np.array(Xs[i])\n",
    "            y_test = np.array(ys[i])\n",
    "            X_train = np.array([*itertools.chain.from_iterable(Xs[:i] + Xs[i + 1:])])\n",
    "            y_train = np.array([*itertools.chain.from_iterable(ys[:i] + ys[i + 1:])])\n",
    "            \n",
    "#             y_test[y_test == 2] = 1\n",
    "#             y_train[y_train == 2] = 1\n",
    "\n",
    "            self._fit(X_train, y_train)\n",
    "            self._evaluate(X_test, y_test)\n",
    "#             ht_result = self.get_results()\n",
    "#             print(ht_result)\n",
    "            self.model = None\n",
    "            \n",
    "        print('100% done')\n",
    "        print('Total acc: {}'.format(self.acc / float(self.counter)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def averageSimilarity(tokens):\n",
    "    counter = 0; suma = 0\n",
    "    \n",
    "    for w1, w2 in itertools.combinations(tokens, 2):\n",
    "        w1 = w1.lower(); w2 = w2.lower()\n",
    "        if w1 not in vectors.vocab or w2 not in vectors.vocab:\n",
    "            continue\n",
    "        suma += vectors.similarity(w1, w2)\n",
    "        counter += 1\n",
    "        \n",
    "    if counter != 0:\n",
    "        return suma /  counter\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractFeatures(tokens):\n",
    "    \n",
    "    words_num = len(tokens)\n",
    "    chars_num = np.sum(np.array([len(token) for token in tokens]))\n",
    "    unk_num = np.sum(np.array([1 for token in tokens if token.lower() not in vectors.vocab]))\n",
    "    mean_words_len = chars_num / float(words_num)\n",
    "    caps_num = np.sum(np.array([1 for token in tokens if token.isupper()]))\n",
    "    average_similarity = averageSimilarity(tokens)\n",
    "    \n",
    "    return[words_num, chars_num, unk_num, caps_num, average_similarity, mean_words_len]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeHashtag(arr, htf):\n",
    "    regex = ('#' + htf.replace('_', '')).split('.tsv')[0]\n",
    "    if regex in arr:\n",
    "        arr.remove(regex)\n",
    "    elif regex.lower() in arr:\n",
    "        arr.remove(regex.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def convertHashtag(arr, htf):\n",
    "    regex = ('#' + htf.replace('_', '')).split('.tsv')[0]\n",
    "    text = re.findall('[A-Z][^A-Z]*', (htf.replace('_', '')).split('.tsv')[0])\n",
    "    \n",
    "    if regex in arr:\n",
    "        arr.remove(regex)\n",
    "        arr.extend(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 551,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessTweet(tweet, htf):\n",
    "    tknzr = TweetTokenizer()\n",
    "    tokens = tknzr.tokenize(tweet)\n",
    "    \n",
    "    if '@midnight' in tokens: \n",
    "        tokens.remove('@midnight')\n",
    "        \n",
    "    removeHashtag(tokens, htf)\n",
    "    \n",
    "    return np.append(glove_char_embeddings(tokens), extractFeatures(tokens))\n",
    "#     return np.array(extractFeatures(tokens))\n",
    "#     return np.array(glove_char_embeddings(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "def glove_vects( tokens ):\n",
    "    \n",
    "    arr = np.zeros(25)\n",
    "    \n",
    "    for tok in tokens:\n",
    "        try:\n",
    "            arr += vectors[tok]\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "    return np.array(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {},
   "outputs": [],
   "source": [
    "def char_embeddings(tokens):\n",
    "    \n",
    "    tk = Tokenizer(num_words=None, char_level=True, oov_token='UNK')\n",
    "    tk.fit_on_texts(tokens)\n",
    "\n",
    "    # -----------------------Skip part start--------------------------\n",
    "    alphabet = \"ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789,;.!?:'\\\"/\\\\|_@#$%^&*~`+-=<>()[]{}\"\n",
    "    char_dict = {}\n",
    "    for i, char in enumerate(alphabet):\n",
    "        char_dict[char] = i + 1\n",
    "\n",
    "    tk.word_index = char_dict.copy()\n",
    "    tk.word_index[tk.oov_token] = max(char_dict.values()) + 1\n",
    "    # -----------------------Skip part end----------------------------\n",
    "\n",
    "    # Convert string to index\n",
    "    sequences = tk.texts_to_sequences(tokens)\n",
    "\n",
    "    # Padding\n",
    "    input_size = 30\n",
    "    data = pad_sequences(sequences, maxlen=input_size, padding='post')\n",
    "#     return np.array(data, dtype='float32')\n",
    "\n",
    "    # Convert to numpy array\n",
    "    sentence = np.zeros(30)\n",
    "    for word in data:\n",
    "        sentence += np.array(word)\n",
    "        \n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "metadata": {},
   "outputs": [],
   "source": [
    "def glove_char_embeddings(tokens):\n",
    "    arr = np.zeros(300)\n",
    "    \n",
    "    for word in tokens:\n",
    "        for char in word:\n",
    "            try:\n",
    "                arr += char_vectors[char]\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "    return np.array(arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def load_document_chars(data_location, htf):\n",
    "    tweets = []\n",
    "    labels = []\n",
    "    \n",
    "    for line in open(os.path.join(data_location, htf)).readlines():\n",
    "        line_split = line.strip().split('\\t')\n",
    "        tweets.append(line_split[1])\n",
    "        labels.append(int(line_split[2]))\n",
    "\n",
    "    Y = np.array(labels)\n",
    "    X_vects = char_embeddings(tweets)\n",
    "\n",
    "    return {'X_vects': X_vects, 'Y': Y}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_document(data_location, htf):\n",
    "    tweets = []\n",
    "    labels = []\n",
    "    \n",
    "    for line in open(os.path.join(data_location, htf)).readlines():\n",
    "        line_split = line.strip().split('\\t')\n",
    "        tweets.append(line_split[1])\n",
    "        labels.append(int(line_split[2]))\n",
    "\n",
    "    Y = np.array(labels)\n",
    "    X_vects = [preprocessTweet(tweet, htf) for tweet in tweets]\n",
    "\n",
    "    return {'X_vects': X_vects, 'Y': Y}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data(data_location):\n",
    "    ht_files = sorted(os.listdir(data_location))\n",
    "\n",
    "    Xs = []\n",
    "    ys = []\n",
    "    ht_list = []\n",
    "    for htf in ht_files:\n",
    "        ht_dict = load_document(data_location, htf)\n",
    "\n",
    "        ht_list.append(htf)\n",
    "        ys.append(ht_dict['Y'])\n",
    "        Xs.append(ht_dict['X_vects'])\n",
    "\n",
    "    return Xs, ys, ht_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Xs, ys, ht_list = create_data('train_data')\n",
    "\n",
    "pairs_num = 0; bigger_first = 0\n",
    "\n",
    "for X, y in zip(Xs, ys):\n",
    "    X_win, X_top10, X_rest = separate_data(np.array(X)[:, 29], y)\n",
    "    \n",
    "    for tweet_pair in itertools.product(X_top10, X_rest):\n",
    "        \n",
    "        if tweet_pair[0] == 0:\n",
    "            continue\n",
    "            \n",
    "        pairs_num += 1\n",
    "        if tweet_pair[0] < tweet_pair[1]:\n",
    "            bigger_first += 1\n",
    "\n",
    "print(bigger_first / pairs_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 552,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0% done\n",
      "Canadian_Superheroes.tsv\n",
      "pairwise_accuracy 0.621683093252464\n",
      "0.06666666666666667% done\n",
      "Comic_Book_Songs.tsv\n",
      "pairwise_accuracy 0.5597964376590331\n",
      "0.13333333333333333% done\n",
      "Elderly_Movies.tsv\n",
      "pairwise_accuracy 0.7243642329778507\n",
      "0.2% done\n",
      "Florida_A_Movie.tsv\n",
      "pairwise_accuracy 0.6301040832666133\n",
      "0.26666666666666666% done\n",
      "Got_Fired_Because.tsv\n",
      "pairwise_accuracy 0.6345609065155807\n",
      "0.3333333333333333% done\n",
      "Gritty_Seuss.tsv\n",
      "pairwise_accuracy 0.5789473684210527\n",
      "0.4% done\n",
      "Hangover_Songs.tsv\n",
      "pairwise_accuracy 0.5738858483189992\n",
      "0.4666666666666667% done\n",
      "Hip_Hop_Star_Wars.tsv\n",
      "pairwise_accuracy 0.5987488829311886\n",
      "0.5333333333333333% done\n",
      "Holiday_Celebs.tsv\n",
      "pairwise_accuracy 0.4844995571302037\n",
      "0.6% done\n",
      "Make_Baseball_Exciting.tsv\n",
      "pairwise_accuracy 0.6318147871545929\n",
      "0.6666666666666666% done\n",
      "Nerd_Broadway.tsv\n",
      "pairwise_accuracy 0.5295646523716699\n",
      "0.7333333333333333% done\n",
      "One_Letter_Off_Foods.tsv\n",
      "pairwise_accuracy 0.5374310480693459\n",
      "0.8% done\n",
      "Political_Superheroes.tsv\n",
      "pairwise_accuracy 0.8859060402684564\n",
      "0.8666666666666667% done\n",
      "Sexy_Holidays.tsv\n",
      "pairwise_accuracy 0.5616\n",
      "0.9333333333333333% done\n",
      "Spring_Break_In_4_Words.tsv\n",
      "pairwise_accuracy 0.554559043348281\n",
      "100% done\n",
      "Total acc: 0.6071643987790221\n"
     ]
    }
   ],
   "source": [
    "Xs, ys, ht_list = create_data('semi_data')\n",
    "clf = XGBoost().run(Xs, ys, ht_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
