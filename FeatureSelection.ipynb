{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import multiprocessing\n",
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gensim GloVe word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\n"
     ]
    }
   ],
   "source": [
    "# from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "# glove2word2vec(glove_input_file=\"glove.twitter.27B/glove.twitter.27B.25d.txt\",\n",
    "#                word2vec_output_file=\"gensim_glove_vectors25.txt\")\n",
    "print('a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-50-f71160a6acc8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeyedvectors\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mKeyedVectors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mglove_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKeyedVectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_word2vec_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../gensim_glove_vectors25.txt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mvectors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mglove_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mload_word2vec_format\u001b[0;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype)\u001b[0m\n\u001b[1;32m   1117\u001b[0m         return _load_word2vec_format(\n\u001b[1;32m   1118\u001b[0m             \u001b[0mWord2VecKeyedVectors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfvocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0municode_errors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0municode_errors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1119\u001b[0;31m             limit=limit, datatype=datatype)\n\u001b[0m\u001b[1;32m   1120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1121\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_keras_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_embeddings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/gensim/models/utils_any2vec.py\u001b[0m in \u001b[0;36m_load_word2vec_format\u001b[0;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype)\u001b[0m\n\u001b[1;32m    223\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"invalid vector on line %s (is this really the text format?)\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mline_no\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m                 \u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdatatype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 225\u001b[0;31m                 \u001b[0madd_word\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    226\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m         logger.info(\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/gensim/models/utils_any2vec.py\u001b[0m in \u001b[0;36madd_word\u001b[0;34m(word, weights)\u001b[0m\n\u001b[1;32m    182\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0madd_word\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m             \u001b[0mword_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 184\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    185\u001b[0m                 \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"duplicate word '%s' in %s, ignoring all but first\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m                 \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "glove_model = KeyedVectors.load_word2vec_format(\"../gensim_glove_vectors25.txt\", binary=False)\n",
    "vectors = glove_model.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "# glove2word2vec(glove_input_file=\"../char-embeddings.txt\",\n",
    "#                word2vec_output_file=\"../gensim_char-embeddings.txt\")\n",
    "\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "char_embeddings = KeyedVectors.load_word2vec_format(\"../gensim_char-embeddings.txt\", binary=False)\n",
    "char_vectors = char_embeddings.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('lil', 0.9541467428207397),\n",
       " ('bitch', 0.9520374536514282),\n",
       " ('mf', 0.9506527781486511),\n",
       " ('bruh', 0.9500958919525146),\n",
       " ('dawg', 0.9483038187026978),\n",
       " ('fuckin', 0.9452493786811829),\n",
       " ('shawty', 0.9435693025588989),\n",
       " ('ass', 0.9413027763366699),\n",
       " ('cuz', 0.9359691143035889),\n",
       " ('shit', 0.9340022802352905)]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_model.wv.similar_by_word('nigga')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.02522685168689944"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectors.similarity('nigga', 'alen')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "import itertools\n",
    "from sklearn.ensemble import AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def separate_data(X, y):\n",
    "        X_win = X[y == 2]\n",
    "        X_top10 = X[y == 1]\n",
    "        X_rest = X[(y != 1) & (y != 2)]\n",
    "        return X_win, X_top10, X_rest\n",
    "    \n",
    "def create_pairwise_data(Xs, ys):\n",
    "        X_pairs = []\n",
    "        y_pairs = []\n",
    "        for X, y in zip(Xs, ys):\n",
    "            X_win, X_top10, X_rest = separate_data(X, [y])\n",
    "\n",
    "            for tweet_pair in itertools.product(X_win, X_top10):\n",
    "                if random() > 0.5:\n",
    "                    tweet_data = np.hstack((tweet_pair[0], tweet_pair[1]))\n",
    "                    tweet_label = 1\n",
    "                else:\n",
    "                    tweet_data = np.hstack((tweet_pair[1], tweet_pair[0]))\n",
    "                    tweet_label = 0\n",
    "\n",
    "                X_pairs.append(tweet_data)\n",
    "                y_pairs.append(tweet_label)\n",
    "\n",
    "            for tweet_pair in itertools.product(X_top10, X_rest):\n",
    "                if random() > 0.5:\n",
    "                    tweet_data = np.hstack((tweet_pair[0], tweet_pair[1]))\n",
    "                    tweet_label = 1\n",
    "                else:\n",
    "                    tweet_data = np.hstack((tweet_pair[1], tweet_pair[0]))\n",
    "                    tweet_label = 0\n",
    "\n",
    "                X_pairs.append(tweet_data)\n",
    "                y_pairs.append(tweet_label)\n",
    "\n",
    "            for tweet_pair in itertools.product(X_win, X_rest):\n",
    "                if random() > 0.5:\n",
    "                    tweet_data = np.hstack((tweet_pair[0], tweet_pair[1]))\n",
    "                    tweet_label = 1\n",
    "                else:\n",
    "                    tweet_data = np.hstack((tweet_pair[1], tweet_pair[0]))\n",
    "                    tweet_label = 0\n",
    "\n",
    "                X_pairs.append(tweet_data)\n",
    "                y_pairs.append(tweet_label)\n",
    "\n",
    "        X = np.vstack(X_pairs)\n",
    "        y = np.array(y_pairs)\n",
    "\n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "from xgboost import XGBClassifier\n",
    "from xgboost import plot_importance\n",
    "from matplotlib import pyplot\n",
    "from random import random\n",
    "\n",
    "class XGBoost(object):\n",
    "    def __init__(self):\n",
    "        self.model = None\n",
    "        self.acc = 0\n",
    "        self.counter = 0\n",
    "\n",
    "    def _create_classifier(self):\n",
    "        self.model = XGBClassifier()\n",
    "#         self.model = LogisticRegression()\n",
    "        \n",
    "    def _fit(self, X, y):\n",
    "        if self.model is None:\n",
    "            self._create_classifier()\n",
    "        self.model.fit(X, y)\n",
    "        \n",
    "#         plot_importance(self.model)\n",
    "#         pyplot.show()\n",
    "#         print(self.model.get_booster().get_score(importance_type = 'total_gain'))\n",
    "        \n",
    "    def _evaluate_pairwise(self, X, y):\n",
    "        X_pairs = []\n",
    "        y_pairs = []\n",
    "        X_win, X_top10, X_rest = separate_data(X, y)\n",
    "        \n",
    "        count_corect = 0\n",
    "        count_total = 0\n",
    "        \n",
    "        for tweet_pair in itertools.product(X_top10, X_rest):\n",
    "            count_total += 1\n",
    "            \n",
    "            win_zeroclass_prob = self.model.predict_proba([tweet_pair[0]])[:,0]\n",
    "            rest_zeroclass_prob = self.model.predict_proba([tweet_pair[1]])[:,0]\n",
    "            \n",
    "            if win_zeroclass_prob < rest_zeroclass_prob:\n",
    "                count_corect += 1\n",
    "                \n",
    "        for tweet_pair in itertools.product(X_win, X_rest):\n",
    "            count_total += 1\n",
    "            \n",
    "            win_zeroclass_prob = self.model.predict_proba([tweet_pair[0]])[:,0]\n",
    "            rest_zeroclass_prob = self.model.predict_proba([tweet_pair[1]])[:,0]\n",
    "            \n",
    "            if win_zeroclass_prob < rest_zeroclass_prob:\n",
    "                count_corect += 1\n",
    "                \n",
    "        for tweet_pair in itertools.product(X_win, X_top10):\n",
    "            count_total += 1\n",
    "            \n",
    "            win_zeroclass_prob = self.model.predict_proba([tweet_pair[0]])[:,0]\n",
    "            rest_zeroclass_prob = self.model.predict_proba([tweet_pair[1]])[:,0]\n",
    "            \n",
    "            if win_zeroclass_prob < rest_zeroclass_prob:\n",
    "                count_corect += 1\n",
    "                \n",
    "        accuracy = count_corect / float(count_total)\n",
    "                \n",
    "        self.counter += 1\n",
    "        self.acc += accuracy\n",
    "                \n",
    "        print('pairwise_accuracy', accuracy)\n",
    "        \n",
    "    def _evaluate(self, X, y):\n",
    "#         np.set_printoptions(threshold=sys.maxsize)\n",
    "        self._evaluate_pairwise(X, y)\n",
    "#         y_pred = self.model.predict(X)\n",
    "#         acc = accuracy_score(y, y_pred)\n",
    "#         self.results = {'accuracy': acc}\n",
    "        \n",
    "    def get_results(self):\n",
    "        return self.results\n",
    "    \n",
    "    def run(self, Xs, ys, ht_list):\n",
    "        num_hts = len(ys)\n",
    "        for i in range(num_hts):\n",
    "            print(str(i / float(num_hts))+'% done')\n",
    "            print(ht_list[i])\n",
    "            \n",
    "            X_test = np.array(Xs[i])\n",
    "            y_test = np.array(ys[i])\n",
    "            X_train = np.array([*itertools.chain.from_iterable(Xs[:i] + Xs[i + 1:])])\n",
    "            y_train = np.array([*itertools.chain.from_iterable(ys[:i] + ys[i + 1:])])\n",
    "            \n",
    "#             y_test[y_test == 2] = 1\n",
    "#             y_train[y_train == 2] = 1\n",
    "\n",
    "            self._fit(X_train, y_train)\n",
    "            self._evaluate(X_test, y_test)\n",
    "#             ht_result = self.get_results()\n",
    "#             print(ht_result)\n",
    "            self.model = None\n",
    "            \n",
    "        print('100% done')\n",
    "        print('Total acc: {}'.format(self.acc / float(self.counter)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def averageSimilarity(tokens):\n",
    "    counter = 0; suma = 0\n",
    "    \n",
    "    for w1, w2 in itertools.combinations(tokens, 2):\n",
    "        w1 = w1.lower(); w2 = w2.lower()\n",
    "        if w1 not in vectors.vocab or w2 not in vectors.vocab:\n",
    "            continue\n",
    "        suma += vectors.similarity(w1, w2)\n",
    "        counter += 1\n",
    "        \n",
    "    if counter != 0:\n",
    "        return suma /  counter\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractFeatures(tokens):\n",
    "    \n",
    "    words_num = len(tokens)\n",
    "    chars_num = np.sum(np.array([len(token) for token in tokens]))\n",
    "    unk_num = np.sum(np.array([1 for token in tokens if token.lower() not in vectors.vocab]))\n",
    "#     mean_words_len = chars_num / float(words_num)\n",
    "    caps_num = np.sum(np.array([1 for token in tokens if token.isupper()]))\n",
    "    average_similarity = averageSimilarity(tokens)\n",
    "    \n",
    "    return[words_num, chars_num, unk_num, caps_num, average_similarity]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeHashtag(arr, htf):\n",
    "    regex = ('#' + htf.replace('_', '')).split('.tsv')[0]\n",
    "    if regex in arr:\n",
    "        arr.remove(regex)\n",
    "    elif regex.lower() in arr:\n",
    "        arr.remove(regex.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def convertHashtag(arr, htf):\n",
    "    regex = ('#' + htf.replace('_', '')).split('.tsv')[0]\n",
    "    text = re.findall('[A-Z][^A-Z]*', (htf.replace('_', '')).split('.tsv')[0])\n",
    "    \n",
    "    if regex in arr:\n",
    "        arr.remove(regex)\n",
    "        arr.extend(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessTweet(tweet, htf):\n",
    "    tknzr = TweetTokenizer()\n",
    "    tokens = tknzr.tokenize(tweet)\n",
    "    \n",
    "    if '@midnight' in tokens: \n",
    "        tokens.remove('@midnight')\n",
    "        \n",
    "    removeHashtag(tokens, htf)\n",
    "    \n",
    "    return np.append(glove_char_embeddings(tokens), extractFeatures(tokens))\n",
    "#     return np.array(extractFeatures(tokens))\n",
    "#     return np.array(glove_char_embeddings(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def glove_vects( tokens ):\n",
    "    \n",
    "    arr = np.zeros(25)\n",
    "    \n",
    "    for tok in tokens:\n",
    "        try:\n",
    "            arr += vectors[tok]\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "    return np.array(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def glove_char_embeddings(tokens):\n",
    "    arr = np.zeros(300)\n",
    "    \n",
    "    for word in tokens:\n",
    "        for char in word:\n",
    "            try:\n",
    "                arr += char_vectors[char]\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "    return np.array(arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_document(data_location, htf):\n",
    "    tweets = []\n",
    "    labels = []\n",
    "    \n",
    "    for line in open(os.path.join(data_location, htf)).readlines():\n",
    "        line_split = line.strip().split('\\t')\n",
    "        tweets.append(line_split[1])\n",
    "        labels.append(int(line_split[2]))\n",
    "\n",
    "    Y = np.array(labels)\n",
    "    X_vects = [preprocessTweet(tweet, htf) for tweet in tweets]\n",
    "\n",
    "    return {'X_vects': X_vects, 'Y': Y}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data(data_location):\n",
    "    ht_files = sorted(os.listdir(data_location))\n",
    "\n",
    "    Xs = []\n",
    "    ys = []\n",
    "    ht_list = []\n",
    "    for htf in ht_files:\n",
    "        ht_dict = load_document(data_location, htf)\n",
    "\n",
    "        ht_list.append(htf)\n",
    "        ys.append(ht_dict['Y'])\n",
    "        Xs.append(ht_dict['X_vects'])\n",
    "\n",
    "    return Xs, ys, ht_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-71-c82af93180e1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mXs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mht_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../train_data'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mclf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mXGBoost\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mht_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-66-ce2370455148>\u001b[0m in \u001b[0;36mcreate_data\u001b[0;34m(data_location)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mht_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mhtf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mht_files\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mht_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_document\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhtf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mht_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhtf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-65-6c3c266c01f6>\u001b[0m in \u001b[0;36mload_document\u001b[0;34m(data_location, htf)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mX_vects\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mpreprocessTweet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtweet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhtf\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtweet\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtweets\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'X_vects'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mX_vects\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Y'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-65-6c3c266c01f6>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mX_vects\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mpreprocessTweet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtweet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhtf\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtweet\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtweets\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'X_vects'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mX_vects\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Y'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-62-84dd153596a0>\u001b[0m in \u001b[0;36mpreprocessTweet\u001b[0;34m(tweet, htf)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mremoveHashtag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhtf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mglove_char_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextractFeatures\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;31m#     return np.array(extractFeatures(tokens))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m#     return np.array(glove_char_embeddings(tokens))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-64-cf18dc6a5588>\u001b[0m in \u001b[0;36mglove_char_embeddings\u001b[0;34m(tokens)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mchar\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m                 \u001b[0marr\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mchar_vectors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mchar\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "Xs, ys, ht_list = create_data('../train_data')\n",
    "\n",
    "clf = XGBoost().run2(Xs, ys, ht_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
